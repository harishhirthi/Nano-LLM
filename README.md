# Nano-LLM
Interesting Topic as a part of Generative AI to generate context for the given prompt.

## Description:
Large Language models are one of the Generative AI, a type of Language Model that generates text, generate code, summarizes a text, language translation and so on. Today, these LLM's are revolutionizing and modernizing the chatbots like OpenAI's ChatGPT, Google's BARD, Meta's Code Llama and so on that helps in assisting the humans in their daily tasks. These Language models are possible today, because of the prominent architecture proposed during 2017's known as  **Transformer**. 

Transformer comprises of *Encoder* and *Decoder* parts. Encoder encodes the Context into vectors and Decoder decodes the context vector to generate text. These Transformer process the data parallely unlike recurrent modles. Now these Transformers are not only used in Natural Language but also for Images, Audio and Multi-modal data.

This LLM is based on Decoder-only Architecture to generate text from the given prompt. This architecture is based on the paper Attention is All you Need.

![Decoder](https://github.com/harishhirthi/Nano-LLM/assets/43694283/0baf6433-f9fb-4429-a526-40e39c1f49bb)

## Dataset:
[BBC News Articles](https://www.kaggle.com/datasets/pariza/bbc-news-summary)

[Avengers and Iron Man Dialogues](https://www.kaggle.com/datasets/divaxshah/avengers-and-iron-man-movies-dataset)

[Shakespeare Plays](https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays)

[Book](Data/Book.txt)

## Contains one file and two folders:
* Nano_LLM.ipynb - Notebook about LLM for the given prompt
* Images
* Data.


